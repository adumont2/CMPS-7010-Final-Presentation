{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13zeGS1NbSVwcNsweGAeVw8CGOsxtVMd2","timestamp":1764452358724}],"gpuType":"A100","authorship_tag":"ABX9TyPVWTb7BUW2913OEG66L8V5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9cAzXuBIKqoi"},"outputs":[],"source":["# @title 1. Install Dependencies\n","# We install 'be-great' for the LLM augmentation and 'kagglehub' to fetch the dataset.\n","!pip install be-great kagglehub xgboost scikit-learn pandas matplotlib -q\n","\n","print(\"‚úÖ Libraries installed successfully.\")"]},{"cell_type":"code","source":["# @title 2. Load and Preprocess Data (FULL FEATURES)\n","import kagglehub\n","import pandas as pd\n","import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","# 1. Download\n","path = kagglehub.dataset_download(\"rabieelkharoua/alzheimers-disease-dataset\")\n","csv_path = os.path.join(path, \"alzheimers_disease_data.csv\")\n","df = pd.read_csv(csv_path)\n","\n","# 2. PREPROCESSING: KEEP EVERYTHING EXCEPT IDs\n","# We allow ADL and FunctionalAssessment back in.\n","# This gives the model very strong clues.\n","drop_cols = ['PatientID', 'DoctorInCharge']\n","\n","existing_drop_cols = [c for c in drop_cols if c in df.columns]\n","df = df.drop(columns=existing_drop_cols)\n","\n","target_col = 'Diagnosis'\n","\n","print(f\"‚úÖ Data Loaded (Full Feature Mode). Shape: {df.shape}\")\n","print(f\"   Features: {len(df.columns)}\")"],"metadata":{"id":"UymxovQnLG-2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 3. Simulate Data Scarcity (N=40)\n","\n","# With ADL included, even 40 patients might be enough for a good baseline.\n","df_small, df_holdout = train_test_split(\n","    df,\n","    train_size=40,\n","    random_state=42,\n","    stratify=df[target_col]\n",")\n","\n","X_small = df_small.drop(columns=[target_col])\n","y_small = df_small[target_col]\n","\n","# 80/20 split within the small dataset\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_small, y_small, test_size=0.2, random_state=42, stratify=y_small\n",")\n","\n","print(f\"üìâ Simulation Setup:\")\n","print(f\"   Training Samples: {len(X_train)}\")\n","print(f\"   Test Samples: {len(X_test)}\")"],"metadata":{"id":"FIS_Hk2LLJ5Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 4. Train Baseline Model (No Augmentation)\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n","\n","# Initialize XGBoost\n","baseline_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n","\n","# Train on the small dataset\n","baseline_model.fit(X_train, y_train)\n","\n","# Evaluate\n","y_pred_base = baseline_model.predict(X_test)\n","y_prob_base = baseline_model.predict_proba(X_test)[:, 1]\n","\n","acc_base = accuracy_score(y_test, y_pred_base)\n","auc_base = roc_auc_score(y_test, y_prob_base)\n","\n","print(f\"üìä Baseline Results (Small Data):\")\n","print(f\"   Accuracy: {acc_base:.4f}\")\n","print(f\"   AUC:      {auc_base:.4f}\")"],"metadata":{"id":"OOkZmUs2LPhR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 5. Run GReaT Augmentation (Filter & Balance Strategy)\n","from be_great import GReaT\n","\n","# 1. Initialize\n","print(\"ü§ñ Initializing GReaT Transformer...\")\n","great_model = GReaT(llm='distilgpt2', batch_size=8, epochs=150)\n","\n","# 2. Train\n","train_combined = pd.concat([X_train, y_train], axis=1)\n","print(\"‚öôÔ∏è Fine-tuning LLM (2-3 mins)...\")\n","great_model.fit(train_combined)\n","\n","# 3. MASSIVE GENERATION & FILTERING\n","# We generate 300 random samples, then filter to get the specific classes we need.\n","print(\"‚ú® Generating 300 candidate samples (to filter for balance)...\")\n","\n","# Use a larger max_length to ensure no cut-offs\n","synthetic_candidates = great_model.sample(\n","    n_samples=300,\n","    max_length=2000,\n","    device=\"cuda\",\n","    guided_sampling=True\n",")\n","\n","# Post-processing\n","synthetic_candidates = synthetic_candidates.dropna()\n","# Ensure columns match\n","synthetic_candidates = synthetic_candidates[train_combined.columns]\n","\n","# 4. CREATE BALANCED DATASET\n","# We grab all the \"Diagnosis=1\" (Disease) patients we found\n","syn_disease = synthetic_candidates[synthetic_candidates[target_col] == 1]\n","\n","# We grab an equal number of \"Diagnosis=0\" (Healthy) patients\n","syn_healthy = synthetic_candidates[synthetic_candidates[target_col] == 0]\n","\n","# Balance them (Cap at 50 each to match our target)\n","n_per_class = 50\n","syn_disease = syn_disease.head(n_per_class)\n","syn_healthy = syn_healthy.head(n_per_class)\n","\n","# Combine\n","synthetic_data = pd.concat([syn_disease, syn_healthy])\n","\n","# Separate X and y\n","X_syn = synthetic_data.drop(columns=[target_col])\n","y_syn = synthetic_data[target_col]\n","\n","print(f\"\\n‚úÖ Balanced Set Created:\")\n","print(f\"   - Synthetic Alzheimer's Cases: {len(syn_disease)}\")\n","print(f\"   - Synthetic Healthy Cases: {len(syn_healthy)}\")\n","print(f\"   - Total Synthetic Samples: {len(synthetic_data)}\")\n","synthetic_data.head()"],"metadata":{"id":"jREsU-v8LRn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 6. Train Augmented Model & Compare Results (Random Forest)\n","from sklearn.ensemble import RandomForestClassifier # <--- SWITCHING MODEL\n","\n","# 1. Augment the Training Set\n","X_aug = pd.concat([X_train, X_syn])\n","y_aug = pd.concat([y_train, y_syn])\n","\n","# 2. Train New Model (Using Random Forest this time)\n","# RF benefits more from row-sampling augmentation\n","baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","baseline_model.fit(X_train, y_train)\n","\n","aug_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","aug_model.fit(X_aug, y_aug)\n","\n","# 3. Evaluate on the SAME Test Set\n","# Baseline Predictions\n","y_pred_base = baseline_model.predict(X_test)\n","y_prob_base = baseline_model.predict_proba(X_test)[:, 1]\n","acc_base = accuracy_score(y_test, y_pred_base)\n","auc_base = roc_auc_score(y_test, y_prob_base)\n","\n","# Augmented Predictions\n","y_pred_aug = aug_model.predict(X_test)\n","y_prob_aug = aug_model.predict_proba(X_test)[:, 1]\n","acc_aug = accuracy_score(y_test, y_pred_aug)\n","auc_aug = roc_auc_score(y_test, y_prob_aug)\n","\n","# 4. Display Comparison\n","print(\"=\"*40)\n","print(f\"üî¨ FINAL RESULTS COMPARISON (Random Forest)\")\n","print(\"=\"*40)\n","print(f\"{'Metric':<15} | {'Baseline':<10} | {'Augmented':<10}\")\n","print(\"-\" * 45)\n","print(f\"{'Accuracy':<15} | {acc_base:.4f}     | {acc_aug:.4f}\")\n","print(f\"{'AUC':<15}      | {auc_base:.4f}     | {auc_aug:.4f}\")\n","print(\"-\" * 45)\n","\n","improvement = (auc_aug - auc_base) * 100\n","print(f\"\\nüöÄ Improvement (AUC): {improvement:+.2f}%\")"],"metadata":{"id":"se2nBrJVLT5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 7. Visualize Results for Presentation\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Data for plotting\n","metrics = ['Accuracy', 'AUC']\n","baseline_scores = [acc_base, auc_base]\n","augmented_scores = [acc_aug, auc_aug]\n","\n","x = np.arange(len(metrics))\n","width = 0.35\n","\n","fig, ax = plt.subplots(figsize=(8, 5))\n","rects1 = ax.bar(x - width/2, baseline_scores, width, label='Baseline (Small Data)', color='#d62728')\n","rects2 = ax.bar(x + width/2, augmented_scores, width, label='Augmented (GReaT)', color='#2ca02c')\n","\n","# Add text labels\n","ax.set_ylabel('Score')\n","ax.set_title('Impact of LLM Data Augmentation on Alzheimer\\'s Prediction')\n","ax.set_xticks(x)\n","ax.set_xticklabels(metrics)\n","ax.set_ylim(0, 1.1)\n","ax.legend(loc='lower right')\n","\n","def autolabel(rects):\n","    for rect in rects:\n","        height = rect.get_height()\n","        ax.annotate(f'{height:.2f}',\n","                    xy=(rect.get_x() + rect.get_width() / 2, height),\n","                    xytext=(0, 3),  # 3 points vertical offset\n","                    textcoords=\"offset points\",\n","                    ha='center', va='bottom', fontweight='bold')\n","\n","autolabel(rects1)\n","autolabel(rects2)\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"1aVj1Z2aLV2v"},"execution_count":null,"outputs":[]}]}